{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. The posterior distribution over mixture models is to essentially integrate over all model uncertainties, as to average over all posterior probabilites. From bayesian perspective, one should not pick the 'best' model (with the highest posterior probability), but to average different models (a stack of models) and weight each of them by their posterior probabilities, that is to count every model, but to note the fact that the ones with less posterior probabilities speaks less. For computing posterior of a mixture model, the distribution and relevant parameters of the mixture model needs to be specified. After that, one can compute the posterior and use it to predict new data.\n",
        "\n",
        "2. The parameters that needs to be specified for computing postperior predictive distribution in general is less than the mixture models, therefore it is less complex in implementation. The resulting posterior predictive distribution is the likelihood times the posteriors of the hyperparameters.\n",
        "\n",
        "3. Include both the missing parts and the observed parts of $X$ into the regression model of $Y$. Treat missing/unobserved parts as latent variables and include those variables along as the parameters of the model. After conducting posterior inference, infer on those latent variables, to make an educated guess on what the true data would be, in the missing rows of $X$."
      ],
      "metadata": {
        "id": "JTLlumCxKiDX"
      }
    }
  ]
}